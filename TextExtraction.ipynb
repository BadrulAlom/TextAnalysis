{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\n",
    "\n",
    "\n",
    "This is an example of applying :class:`sklearn.decomposition.NMF` and\n",
    ":class:`sklearn.decomposition.LatentDirichletAllocation` on a corpus\n",
    "of documents and extract additive models of the topic structure of the\n",
    "corpus.  The output is a list of topics, each represented as a list of\n",
    "terms (weights are not shown).\n",
    "\n",
    "Non-negative Matrix Factorization is applied with two different objective\n",
    "functions: the Frobenius norm, and the generalized Kullback-Leibler divergence.\n",
    "The latter is equivalent to Probabilistic Latent Semantic Indexing.\n",
    "\n",
    "The default parameters (n_samples / n_features / n_components) should make\n",
    "the example runnable in a couple of tens of seconds. You can try to\n",
    "increase the dimensions of the problem, but be aware that the time\n",
    "complexity is polynomial in NMF. In LDA, the time complexity is\n",
    "proportional to (n_samples * iterations).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "root = \"C:/Users/Badrul/Documents/DataScience/TextAnalysis\"\n",
    "rawDataPath = root + \"/Data/RawData/20news-bydate-train\"\n",
    "dictPath = root + \"/Data/tmp/datadictionary.dict\"\n",
    "corpusPath = root + \"/Data/tmp/corpus.mm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "dataset = load_files(rawDataPath,description= None,load_content = True, \n",
    "                           encoding='latin1', decode_error='strict', shuffle=True, \n",
    "                           random_state=42)\n",
    "#text_test_subset = text_train_subset # load your actual test data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 0.001s.\n"
     ]
    }
   ],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "#dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "#                            remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 0.360s.\n",
      "Extracting tf features for LDA...\n",
      "done in 0.349s.\n",
      "\n",
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 0.110s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: edu university article writes just posting nntp host don think like people know cc state good cs right time new\n",
      "Topic #1: com writes article netcom hp ibm sun access att posting digex nntp corporation dave host systems corp reply don world\n",
      "Topic #2: god bible jesus people believe christ church christian faith christians truth life say hell religion christianity brian says evidence think\n",
      "Topic #3: windows card dos file drive pc files thanks use scsi disk video help problem using drivers graphics version software program\n",
      "Topic #4: ca team canada nhl players game hockey bnr cup toronto play season teams win year cs vancouver league university writes\n",
      "Topic #5: key chip clipper encryption keys des public algorithm government nsa security privacy use secure administration technology used bit enforcement serial\n",
      "Topic #6: pitt cs gordon banks edu pittsburgh science univ soon computer reply dept article utexas uucp writes colorado obviously probably insurance\n",
      "Topic #7: uk ac tony 44 email atheism cramer michael ve reply group mail help running university image work peter sci file\n",
      "Topic #8: armenian turkish armenians turks serdar argic armenia soviet genocide people muslim russian uucp today population army war women government world\n",
      "Topic #9: nasa gov space brian research ___ orbit __ center moon scott cost software org writes _____ article world apple oil\n",
      "\n",
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 2.452s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: edu writes university posting article nntp host just think usa like know does don sure really state distribution want way\n",
      "Topic #1: com like systems reply hp distribution thanks used opinions ca new mail netcom version tin services usa good corporation does\n",
      "Topic #2: writes people think say com way god said don time article point believe question really world just like know did\n",
      "Topic #3: thanks university windows use help using know set problem advance running does work card question mail need pc program files\n",
      "Topic #4: ca university year win game team toronto play time great teams won years players games ve don canada did ll\n",
      "Topic #5: use used years people public make time like just long work government things want probably right small yes key possible\n",
      "Topic #6: cs science writes computer dept reply pittsburgh article univ sci pitt information simple send soon does 17 uk uucp andrew\n",
      "Topic #7: posting nntp host university writes edu reply uk technology article institute sun mail 13 state ac post michael new john\n",
      "Topic #8: world uucp version tin york ways new newsreader uunet wrote work israel population news times women wish com today said\n",
      "Topic #9: posting nntp host world space writes org nasa net access distribution research unix thanks gov usa need 15 virginia read\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
      "done in 3.037s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: mr stephanopoulos gun president law don know going house think tax guns said firearms people congress control weapons states did\n",
      "Topic #1: god 00 jesus bible christian believe christ moral christians ah morality faith religion air church truth evidence christianity atheism does\n",
      "Topic #2: com space access sun launch digex 1993 satellite world commercial earth nasa pat new harvard net article 10 communications ed\n",
      "Topic #3: key use encryption chip des information ibm data used clipper widget bit privacy available mail security internet public anonymous keys\n",
      "Topic #4: edu state ohio andrew acs cs cmu rochester screen university posting nntp host magnus pittsburgh uk pitt science hp article\n",
      "Topic #5: armenian people armenians turkish said men armenia saw soviet turks population killed world war russian government israeli army genocide women\n",
      "Topic #6: ax max a86 b8f pl 34u 34 mr 12 14 edu mb com university distribution reply usa 33 end po\n",
      "Topic #7: edu windows com university posting host nntp use thanks dos does like know card ca scsi drive problem writes article\n",
      "Topic #8: edu com writes article don just people like think know posting good university nntp host time way say ca right\n",
      "Topic #9: file 55 team 10 16 la 15 pts vs 11 12 20 13 players 30 lib 18 pt 24 nhl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Badrul\\Anaconda3\\envs\\kerasenv\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people don just like think good know time right make\n",
      "Topic 1:\n",
      "edu university posting nntp host article distribution writes washington cs\n",
      "Topic 2:\n",
      "com hp article writes ibm sun posting att nntp host\n",
      "Topic 3:\n",
      "drive scsi ide drives disk hard controller mac bus apple\n",
      "Topic 4:\n",
      "game team games year players hockey season play win nhl\n",
      "Topic 5:\n",
      "key clipper chip encryption keys escrow government algorithm security secret\n",
      "Topic 6:\n",
      "uk ac university ed 44 __ newsreader host posting nntp\n",
      "Topic 7:\n",
      "nasa gov space center research ___ __ moon laboratory station\n",
      "Topic 8:\n",
      "cs pitt gordon banks science edu pittsburgh computer univ soon\n",
      "Topic 9:\n",
      "god jesus bible christian christ christians faith believe church christianity\n",
      "Topic 10:\n",
      "windows file dos window files card program use thanks help\n",
      "Topic 11:\n",
      "ca canada bnr university writes article cs toronto posting nntp\n",
      "Topic 12:\n",
      "israel israeli jews turkish armenian arab armenians org jewish armenia\n",
      "Topic 13:\n",
      "ohio state acs edu university john article nntp host posting\n",
      "Topic 14:\n",
      "access digex pat net communications usa com public posting nntp\n",
      "Topic 15:\n",
      "netcom com 408 services david line communications writes brian smith\n",
      "Topic 16:\n",
      "uiuc cso illinois edu news writes university article cars mike\n",
      "Topic 17:\n",
      "cleveland cwru freenet western case edu usa oh previous says\n",
      "Topic 18:\n",
      "cc columbia edu utexas gary buffalo purdue austin au university\n",
      "Topic 19:\n",
      "andrew cmu pittsburgh edu engineering reply host computer nntp posting\n",
      "Topic 0:\n",
      "key chip encryption clipper keys security government public use privacy\n",
      "Topic 1:\n",
      "com writes article posting nntp host hp sun mark colorado\n",
      "Topic 2:\n",
      "ca db canada newsreader columbia version tin se posting host\n",
      "Topic 3:\n",
      "year mr president think clinton baseball going said did bob\n",
      "Topic 4:\n",
      "edu cs university thanks computer know posting mail host nntp\n",
      "Topic 5:\n",
      "nasa gov edu jim __ science pitt ___ article writes\n",
      "Topic 6:\n",
      "10 team game games 17 11 25 12 play hockey\n",
      "Topic 7:\n",
      "145 b8f a86 0d 75u 34u 2di 0t 1d9 g9v\n",
      "Topic 8:\n",
      "edu posting host nntp article university writes state au ohio\n",
      "Topic 9:\n",
      "uk ac apr 1993 93 edu message gmt mit 16\n",
      "Topic 10:\n",
      "ax max g9v pl giz b8f 2tm 1t bhj 3t\n",
      "Topic 11:\n",
      "said turkish armenian people armenians men women went turkey armenia\n",
      "Topic 12:\n",
      "00 sale card dos ms video scott cards monitor offer\n",
      "Topic 13:\n",
      "windows file use program window files available graphics using software\n",
      "Topic 14:\n",
      "god people jesus does believe christian think say bible don\n",
      "Topic 15:\n",
      "space drive scsi mac uiuc disk hard pc data drives\n",
      "Topic 16:\n",
      "don just like know people good writes think time com\n",
      "Topic 17:\n",
      "gun government com public control states guns people american new\n",
      "Topic 18:\n",
      "people israel law jews israeli right rights state government writes\n",
      "Topic 19:\n",
      "edu car apple article writes berkeley cars power washington problem\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "#dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
